# Module 4: Modeling Part 2

## Lecture

### Classification

<iframe width="640" height="360" src="https://www.youtube.com/embed/nH4Bou92FzE?si=tNxvZVaGtF0dbZtg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<br>

<iframe src="https://drive.google.com/file/d/1CfFIOW9UpTAbI6B1aihF7atPWw5OmkXk/preview" width="640" height="480" allow="autoplay"></iframe>

### Clustering

<iframe width="640" height="360" src="https://www.youtube.com/embed/ziVEe99JZXQ?si=9vy4SU6iH3r_1IWE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<br>

<iframe src="https://drive.google.com/file/d/1ua6fFAoelblHpd8ycUQBkh3LVtFX3rON/preview" width="640" height="480" allow="autoplay"></iframe>

## Lab

### Classification

In this example we'll be looking at the Pima Indians Diabetes Database collected by the National Institute of Diabetes and Digestive and Kidney Diseases. The dataset contain 768 observations on 9 variables related to the onset of diabetes in women.

#### Load Libraries

```{r, results='hide', message=FALSE, warning=FALSE}
library(tidyverse) # General functions and plots
library(mlbench) # Source of the dataset
library(glmnet) # For penalized regression
library(pROC) # For AUC calculations
library(randomForest) # AFor the random forest model
```

#### Read in Data

```{r}
data("PimaIndiansDiabetes")

# Set seed for reproducibility
set.seed(123)
```

We'll need to convert the labelled response variable into a numeric binary value.
```{r}
diabetes.data <- PimaIndiansDiabetes %>%
  mutate(diabetes = case_when(diabetes == 'pos' ~ 1,
                              diabetes == 'neg' ~ 0))
```

#### Exploratory Data Analysis

```{r}
diabetes.pc <- diabetes.data  %>%
  mutate(across(c(where(is.numeric)), ~(.x-mean(.x))/sd(.x))) %>%
  dplyr::select(where(is.numeric)) %>%
  cor()

diabetes.pc %>%
  as.data.frame() %>%
  rownames_to_column(var = "Variable_1") %>%
  pivot_longer(cols = -Variable_1, names_to = "Variable_2", values_to = "Correlation") %>%
  ggplot(., aes(x = Variable_1, y = Variable_2, fill = Correlation)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white",
                       midpoint = 0, limit = c(-1, 1), space = "Lab",
                       name = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  coord_fixed() +
  labs(title = "Correlation Matrix of Diabetes Dataset",
       x = "",
       y = "")
```

#### Create Training and Testing Sets

```{r}
# Split training data into explanatory and response variables
Training.dat <- diabetes.data %>%
  slice_sample(prop = 0.75)

Train.X <- Training.dat %>%
  dplyr::select(-diabetes) %>%
  as.matrix()

Train.Y <- Training.dat %>%
  dplyr::select(diabetes) %>%
  as.matrix()

# Do the same for the testing data
Testing.dat <- diabetes.data %>%
  anti_join(., Training.dat)

Test.X <- Testing.dat %>%
  dplyr::select(-diabetes) %>%
  as.matrix()

Test.Y <- Testing.dat %>%
  dplyr::select(diabetes) %>%
  as.matrix()
```

#### Fit a Logistic Regression Model

```{r}
# Fit a Logistic Regression model
logistic.mod <- glm(diabetes ~ ., data = Training.dat, family = "binomial")

# Predict on training and testing sets
logistic.train <- predict(logistic.mod, newdata = Training.dat, type = "response")
logistic.test <- predict(logistic.mod, newdata = Testing.dat, type = "response")

# Calculate AUC for Logistic Regression
logistic.train.auc <- roc(c(Train.Y), c(logistic.train))
logistic.train.auc
logistic.test.auc <- roc(c(Test.Y), c(logistic.test))
logistic.test.auc
```

#### Fit an Elastic Net Model

First, you need to identify an optimal lambda value.

Using cross validation for this helps to identify a lambda value which generalizes well as the best one should perform well across all folds.
```{r}
# Need to conduct cross validation for a more robust estimate or the optimal lambda
foldid <- sample(rep(1:5, length.out = nrow(Training.dat)))

EN.cv.results <- cv.glmnet(Train.X, Train.Y, type.measure="deviance", family="binomial", foldid=foldid, alpha=0.5)

## plot results of 5-fold CV
plot(EN.cv.results)
```

From the deviance-lambda value plot, we see that some amount of regularization is beneficial to model performance, but larger lambda values lead to poor model performance.

Now using the optimal lambda, fit the EN model.
```{r}
EN.mod <- glmnet(Train.X, Train.Y, type.measure="deviance", family="binomial", lambda = EN.cv.results$lambda.min, alpha=0.5)

## Assess the model performance on the training and testing sets
EN.train <- predict(EN.mod, newx = Train.X, type = "response", s=EN.cv.results$lambda.1se)
EN.test <- predict(EN.mod, newx = Test.X, type = "response", s=EN.cv.results$lambda.1se)

## Calculate the auc for each model
EN.train.auc <- roc(c(Train.Y), c(EN.train))
EN.test.auc <- roc(c(Test.Y), c(EN.test))

## Identify and capture the best index for each model
train.snsp <- cbind(EN.train.auc$sensitivities, EN.train.auc$specificities) 
train.snsp.best <- which.max(apply(train.snsp, MARGIN = 1 , FUN = min))

## For the testing set
test.snsp <- cbind(EN.test.auc$sensitivities, EN.test.auc$specificities) 
test.snsp.best <- which.max(apply(test.snsp, MARGIN = 1 , FUN = min))

## Calculate thresholds scores
EN.train.auc.threshold <- EN.train.auc$thresholds[train.snsp.best]
EN.test.auc.threshold <- EN.test.auc$thresholds[test.snsp.best]

# Extract the coefficients for the chosen lambda
coefficients <- coef(EN.mod, s = 'lambda.min')

# Count the number of non-zero coefficients
num_coefficients <- apply(coefficients != 0, 2, sum)

## join results into a dataframe to return
test.Results <- tibble(
  lambda = EN.cv.results$lambda.min,
  train_AUC = as.double(EN.train.auc$auc),
  test_AUC = as.double(EN.test.auc$auc),
  NumCoefficients = num_coefficients
)
test.Results
```

#### Dataset Overview

The `GlobalPatterns` dataset includes:

* **Operational Taxonomic Units (OTUs)**: OTUs are groups of related organisms. The OTU table records the abundance of each OTU in different samples
* **Sample Metadata**: This includes information about each sample, such as its source (e.g., soil, freshwater, human gut), which helps us to interpret the clustering results
* **Phylogenetic Tree**: This tree represents the evolutionary relationships among OTUs
* **Taxonomy Data**: This provides detailed taxonomic classification (e.g., genus, species) for each OTU

We can use these data to cluster samples based on their microbial community composition, allowing us to explore similarities and differences across environmental samples.

### Clustering

#### Load Libraries

```{r, results='hide', message=FALSE, warning=FALSE}
library(tidyverse) # General functions and plots
library(phyloseq) # Data source
library(FNN) # K-Nearest Neighbors
library(cluster) # Hierarchical Clustering
library(ggdendro) # For visualizing dendrograms
```

#### Read in Data

```{r, results='hide', message=FALSE, warning=FALSE}
# Load example microbiome dataset from phyloseq
data(GlobalPatterns)
microbiome.data <- GlobalPatterns

# Set seed for reproducibility
set.seed(123)
```

We need to extract the necessary data for the analysis from the data source. We'll also take a look at the structure of the data.
```{r}
# Extract the Operational Taxonomic Units (OTU) table and sample data
OTU.data <- otu_table(microbiome.data) %>%
  as.data.frame()
Sample.data <- sample_data(microbiome.data)
```

Looking at the structure of the OTU data, we can see that the range of values for the variables are quite large and so we'll need to normalize the data.
```{r}
summary(OTU.data)
```

After normalizing the data the ranges are much more consistent.
```{r}
OTU.scaled <- scale(OTU.data, scale = T, center = T)
summary(OTU.scaled)

# We also need to transpose the data
OTU.scaled <- t(OTU.scaled)
```

Looking at the sample data we get an idea of how the experiment was designed and the different sources of the samples.
```{r}
summary(Sample.data)
```

#### K Means

```{r}
k <- 3 # Lets use 3 clusters 

kmeans.result <- kmeans(OTU.scaled, centers = k, nstart = 10)

as.factor(kmeans.result$cluster)
```

How do we decide how to choose the number of clusters, K? We can calculate the Within-Cluster Sum of Squares (WCSS). A lower WCSS indicates that data points are closer to their cluster centroids, implying more cohesive clusters.
```{r}
# Calculate WCSS for different numbers of clusters - Max is 25 as we have 26 groups and min is 2 since having only 1 group is not useful. 
wcss <- map(2:25, function(k) {
  kmeans(OTU.scaled, centers = k, nstart = 10)$tot.withinss
})

# Plot WCSS against the number of clusters
plot(2:25, wcss, type = "b", pch = 19, frame = FALSE,
     xlab = "Number of Clusters K",
     ylab = "Within-Clusters Sum of Squares",
     main = "Elbow Method for Determining Optimal Number of Clusters")
```

There is no distinct 'elbow' on the plot which makes identifying an optimal number of clusters difficult. We'll refit the model and see how it influences cluster assignments.

Lets set k=15, seems like WCSS doesn't reduce as quickly after that point.
```{r}
kmeans.result <- kmeans(OTU.scaled, centers = 15, nstart = 10)

as.factor(kmeans.result$cluster)
```

These clusters represent groups of samples that are found to be more similar to each other in terms of their OTU abundances.

Let's evaluate using PCA:
```{r}
pca_results <- prcomp(OTU.scaled, center = TRUE, scale. = TRUE)
pca_data <- data.frame(pca_results$x, Cluster = as.factor(kmeans.result$cluster))

ggplot(pca_data, aes(PC1, PC2, color = Cluster)) +
  geom_point(size = 2) +
  labs(title = "PCA of OTU Data with 15 K-Means Clusters")
```

The PCA plot for the 15 cluster K-Means results indicates that we're probably using too many clusters. Lets reassess with only 4 clusters this time.
```{r}
kmeans.result2 <- kmeans(OTU.scaled, centers = 4, nstart = 10)

pca_results2 <- prcomp(OTU.scaled, center = TRUE, scale. = TRUE)
pca_data2 <- data.frame(pca_results2$x, Cluster = as.factor(kmeans.result2$cluster))

ggplot(pca_data2, aes(PC1, PC2, color = Cluster)) +
  geom_point(size = 2) +
  labs(title = "PCA of OTU Data with 4 K-Means Clusters")
```

That didn't cluster the groups as expected,. The proximity of the points on the PCA plot suggests similarity among groups based on the variance captured by the first two PCs, but this may not translate to how the algorithm is determining the number of groups. So, how can we decide what's best?

* Expert opinion
* Silhouette analysis
* Hierarchical clustering

These are just strategies, they don't provide a conclusive "optimal" K value.

Let's explore the silhouette analysis

The Silhouette score measures how similar each point is to its own cluster compared to other clusters. A higher silhouette score suggests better-defined clusters.
```{r}
silhouette.values <- map(2:10, function(k){
  Kmeans.res <- kmeans(OTU.scaled, centers = k, nstart = 10)
  silhouette.score <- silhouette(Kmeans.res$cluster, dist(OTU.scaled))
  mean(silhouette.score[, 3])  # 3rd column is the silhouette width
})

plot(2:10, silhouette.values, type="b", pch = 19, frame = FALSE, 
     xlab="Number of Clusters K",
     ylab="Average Silhouette Width")
```

Looks like 6 might be a good choice for our analysis.

#### Hierarchical Clustering

Hierarchical clustering can help determine where the natural breaks in the data are. We can visually inspect the dendrogram to choose a natural number of clusters.
```{r}
# Perform hierarchical clustering
hc.res <- hclust(dist(OTU.scaled), method = "complete")

# Plot the dendrogram
plot(hc.res, labels = Sample.data$SampleType, main = "Hierarchical Clustering Dendrogram", xlab = "", sub = "")
```

```{r}
# Cut the dendrogram to create group labels
hc.clusters <- cutree(hc.res, k = 7)
hc.clusters
```

#### K-Nearest Neighbours

Typically used for classification, but can be used for clustering.
```{r}
# Number of neighbors to consider
k <- 7

# Fit KNN model
knn_result <- knn(train = OTU.scaled, test = OTU.scaled, cl = Sample.data$SampleType, k = k)

# Add clustering results to sample data
Sample.data$KNN_Cluster <- knn_result

# Visualize clustering result
table(Sample.data$SampleType, Sample.data$KNN_Cluster)[,1:3]
table(Sample.data$SampleType, Sample.data$KNN_Cluster)[,4:6]
```

The numbers in each cell indicate how many samples of a particular SampleType were assigned to a specific KNN_Cluster. Cluster 1 (first column), contains 4 samples originally labeled as "Feces" and 3 samples originally labeled as "Mock". This would be considered a mixed cluster as it contains samples from multiple sample types may indicate that those sample types share similar characteristics in terms of OTU data.

#### Independent Work Section

Load in gene expression data used earlier and clean it to contain only the 1000 most variable genes.
```{r, results='hide', message=FALSE, warning=FALSE}
library(tidyverse)
library(TCGAbiolinks)
library(SummarizedExperiment)
library(glmnet)
library(cluster)
```

You can download the code from the TCGA repository as below, but if that doesn't work or it takes too long you can use the the next block of code to read in the CSV files instead.
```{r, results='hide', message=FALSE, warning=FALSE}
# Lets use the same Gene Expression data as before
query <- GDCquery(project = "TCGA-LAML", 
                 data.category = "Transcriptome Profiling", 
                 data.type = "Gene Expression Quantification", 
                 workflow.type = "STAR - Counts")

GDCdownload(query)
Study.data <- GDCprepare(query)

# Extract the gene expression data
expression.data <- assay(Study.data)

# Transpose the dataset to have samples as rows and genes as columns
expression.data <- t(expression.data)

# Convert to a data frame for easier manipulation
expression.df <- as.data.frame(expression.data)

# Extract the clinical data 
clinical.data <- colData(Study.data)
```

```{r}
# Or instead load in CSV files of the data - This may also take a minute
# You can do this by providing the path to the data or by using read.csv(file.choose()) instead
expression.df <- read.csv('./datasets/clustering_expression_data.csv') %>% # Use your file path for this data
  dplyr::select(-X)

clinical.data <- read.csv('./datasets/clustering_clinical_data.csv') %>% # Use your file path to this data
  dplyr::select(barcode, vital_status, age_at_diagnosis) %>%
  mutate(vital_status = case_when(vital_status == 'Alive' ~ 1,
                                  vital_status == 'Dead' ~ 0))
```

```{r}
# Extract the last four digits of the patient identifiers
patient.labels <- substr(clinical.data$patient, nchar(clinical.data$patient) - 3, nchar(clinical.data$patient))

top.genes <- apply(expression.df, 2, var)
top.genes <- sort(top.genes, decreasing = TRUE)
expression.df <- expression.df[, names(top.genes)[1:1000]]

# Standardize the data
expression.df.scaled <- scale(expression.df) %>%
  as.data.frame() %>%
  dplyr::select(!where(~ sum(is.na(.)) > 0))
```

Conduct K-means clustering as well as hierarchical clustering. How many clusters do you think is optimal?

Create an elbow plot for up to 100 clusters (Or 50 if it takes too long on your computer).
```{r}
wcss <- map(2:50, function(k) {
  kmeans(expression.df.scaled, centers = k, nstart = 25)$tot.withinss
})

# Plot WCSS against the number of clusters
plot(2:50, wcss, type = "b", pch = 19, frame = FALSE,
     xlab = "Number of Clusters K",
     ylab = "Within-Clusters Sum of Squares",
     main = "Elbow Method for Determining Optimal Number of Clusters")
```

PSelect a number of clusters to use based on the elbow plots and conduct PCA to see if that can further inform your decision.
```{r}
Kmean.results.20 <- kmeans(expression.df.scaled, centers = 20, nstart = 25)

pca_results <- prcomp(expression.df.scaled, center = TRUE, scale. = TRUE)
pca_data <- data.frame(pca_results$x, Cluster = as.factor(Kmean.results.20$cluster))

ggplot(pca_data, aes(PC1, PC2, color = Cluster)) +
  geom_point(size = 2) +
  labs(title = "PCA of OTU Data with 15 K-Means Clusters")
```

Not very informative.

Conduct a Silhouette analysis up to the number of clusters identified previously.
```{r}
silhouette.values <- map(2:25, function(k){
  Kmeans.res <- kmeans(expression.df.scaled, centers = k, nstart = 10)
  silhouette.score <- silhouette(Kmeans.res$cluster, dist(expression.df.scaled))
  mean(silhouette.score[, 3])  # 3rd column is the silhouette width
})

plot(2:25, silhouette.values, type="b", pch = 19, frame = FALSE, 
     xlab="Number of Clusters K",
     ylab="Average Silhouette Width")
```

Less than 10 seems to be a good number, lets try 7.

Conduct hierarchical clustering to see if it suggests something different. Use the "patient.labels" object for the labels.
```{r}
# Perform hierarchical clustering
hc.res <- hclust(dist(expression.df.scaled), method = "complete")

# Plot the dendrogram
plot(hc.res, main = "Hierarchical Clustering Dendrogram", xlab = "", sub = "")
```

```{r}
# Cut the dendrogram to create group labels
hc.clusters <- cutree(hc.res, k = 8)
```

Advanced, join the cluster identification data with the Clinical.data from the classification independent work section. Compare the performance of a logistic regression model fitted to this newly joined dataset against the performance of the EN fit using the Merged.data without cluster IDs. How much information loss is caused by reducing the expression data into clusters vs including it directly into the model?
```{r}
# Prepare the data
Expression.data <- Study.data %>%
  assay() %>% # Pull out the data
  t() %>% # Transpose the dataset
  as.data.frame() %>%
  rownames_to_column(var = "barcode")

Clinical.data <- colData(Study.data) %>%
  as.data.frame() %>%
  dplyr::select(barcode, vital_status, age_at_diagnosis) %>%
  mutate(vital_status = case_when(vital_status == 'Alive' ~ 1,
                                  vital_status == 'Dead' ~ 0),
         ClusterID = hc.clusters) ## Add cluster ID to the dataset

# Join gene expression data to the clinical outcomes data 
Merged.data <- Expression.data[,1:1001] %>% # We're only going to use 1000 of the 60,000 genes available (plus the barcode identifier) for computational efficiency
  left_join(Clinical.data, ., by = 'barcode')
```

```{r}
# Split data into training and testing sets
set.seed(123)
Training.data <- Merged.data %>%
  slice_sample(prop = 0.8)

Train.X <- Training.data %>%
  dplyr::select(-vital_status, -ClusterID) %>%
  as.matrix()

Train.Y <- Training.data %>%
  dplyr::select(vital_status) %>%
  as.matrix()

Testing.data <- Merged.data %>%
  anti_join(., Training.data, by = 'barcode')

Test.X <- Testing.data %>%
  dplyr::select(-vital_status, -ClusterID) %>%
  as.matrix()

Test.Y <- Testing.data %>%
  dplyr::select(vital_status) %>%
  as.matrix()
```

#### EN Model

```{r}
# Perform 5-fold cross-validation for Elastic Net
foldid <- sample(rep(1:5, length.out = nrow(Training.data)))

EN.cv.fit <- cv.glmnet(Train.X, Train.Y, type.measure="deviance", family = "binomial", foldid=foldid, alpha = 0.5)
plot(EN.cv.fit)

# Fit the Elastic Net model using the optimal lambda from CV
EN.fit <- glmnet(Train.X, Train.Y, family = "binomial", alpha = 0.5, lambda = EN.cv.fit$lambda.min)

# Predict on training and testing sets
EN.pred.train <- predict(EN.fit, newx = Train.X, type = "response", s = EN.cv.fit$lambda.min)
EN.pred.test <- predict(EN.fit, newx = Test.X, type = "response", s = EN.cv.fit$lambda.min)

# Calculate AUC for Elastic Net
EN.train.auc <- roc(c(Train.Y), c(EN.pred.train))
EN.train.auc
EN.test.auc <- roc(Test.Y, EN.pred.test)
EN.test.auc
```

#### Logistic Regression Model

```{r}
# Fit a Logistic Regression model
logistic.mod <- Training.data %>%
  dplyr::select(colnames(Clinical.data), -barcode) %>%
  glm(vital_status ~ ., data = ., family = "binomial")

# Predict on training and testing sets
logistic.train <- Training.data %>%
  dplyr::select(colnames(Clinical.data), -barcode) %>%
  predict(logistic.mod, newdata = ., type = "response")

logistic.test <- Testing.data %>%
  dplyr::select(colnames(Clinical.data), -barcode) %>%
  predict(logistic.mod, newdata = ., type = "response")

# Calculate AUC for Logistic Regression
logistic.train.auc <- roc(c(Train.Y), c(logistic.train))
logistic.train.auc
logistic.test.auc <- roc(c(Test.Y), c(logistic.test))
logistic.test.auc
```

:::: {.callout type="green" title="Lab Completed!"}

Congratulations! You have completed Lab 4!

::::
